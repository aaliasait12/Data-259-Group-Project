# Data-259-Group-Project
Aalia Sait, Ayesha Ramnani, George Lin 

Our project aims to investigate the question:
Is facial recognition technology inherently biased against certain demographic groups, and what ethical risks arise when it is used in real-world scenarios such as law enforcement or hiring?
We aim to explore how algorithmic bias presents itself in facial recognition, particularly with regards to race and gender, and examine the implications of this for fairness and privacy. 

## We plan to use a combination of quantitative and qualitative analysis to conduct our analysis. 

### Literature reviews:
We will review existing literature and case studies to motivate our study as well as learn more about the ethical implications of biased facial recognition. Some potential examples include:
https://pmc.ncbi.nlm.nih.gov/articles/PMC8320316/
https://journalofethics.ama-assn.org/article/what-are-important-ethical-implications-using-facial-recognition-technology-health-care/2019-02

### Data: 
We aim to analyze publicly available data that provide demographic information (eg: age, gender, and race) for labeled facial images. 
Some possible options include: 
Labeled Faces in the Wild (LFW) dataset
FairFace dataset
This website contains many datasets on facial recognition
We aim to evaluate model performance across demographic groups and see if we observe any disparities. 

### Analysis and Modeling 
We hope to use an open source facial recognition model such as Facenet or pretrained models such as DeepFace to run classification and identification tasks. We will evaluate metrics such as false positives and accuracy across different race and gender groups. 
Our analysis will focus on how biased training data may lead to worse performance for certain groups.


